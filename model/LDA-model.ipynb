{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T11:25:43.680103Z",
     "start_time": "2025-06-29T11:25:43.670930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ],
   "id": "18a5991ea75b9707",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\markl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\markl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\markl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T11:25:43.708971Z",
     "start_time": "2025-06-29T11:25:43.693024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize stopwords and lemmatizer outside the function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords = {\n",
    "    # Domain-specific noise\n",
    "    'http', 'https', 'www', 'com', 'org', 'smilies', 'gif', 'hi', 'hey',\n",
    "    # Forum-specific terms\n",
    "    'vp', 'ps', 'like', 'think', 'know', 'would', 'get', 'go', 'say', 'need', 'want', 'real', 'said', 'yes', 'man', 'make', 'term', 'agw', 'one', 'really', 'even', 'come', 'made', 'link', 'based', 'nothing', 'right', 'mind', 'problem', 'since', 'week',  'sure', 'good', 'kind', 'last', 'true', 'keep', 'also', 'help', 'give', 'mean', 'case', 'best', 'give', 'make', 'go', 'think', 'know', 'like', 'free', 'must', 'else', 'soon', 'read', 'part', 'fact', 'may', 'would', 'could', 'might', 'also', 'even',\n",
    "    # Vague terms\n",
    "    'thing', 'something', 'anything', 'someone', 'way', 'much', 'many',\n",
    "    # Contractions\n",
    "    'don', 't', 'll', 've', 're', 'm', 's', 'd', 'nt'\n",
    "    # Add weak signal words from your topics\n",
    "    'well', 'may', 'claim', 'wrong', 'work',\n",
    "    # Remove question words\n",
    "    'question',\n",
    "    # Temporal words that dilute topics\n",
    "    'time', 'year'\n",
    "}\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "# Special handling for forum text\n",
    "forum_patterns = [\n",
    "    (r'smilies?\\w+\\s?\\w*\\s?\\d+', ''),  # Remove smiley codes\n",
    "    (r'//.*', ''),                      # Remove code-like comments\n",
    "    (r'\\.{2,}', ' '),                   # Replace multiple dots\n",
    "    (r'[^\\w\\s]', ' ')                   # Keep only words and spaces\n",
    "]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "id": "9ccc1cd8545992e6",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T11:25:43.983560Z",
     "start_time": "2025-06-29T11:25:43.719717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the CSV file\n",
    "file_path = \"C:\\\\Users\\\\markl\\\\Downloads\\\\archive\\\\climate_posts_clean.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Get random sample of 250 entries\n",
    "    sample_size = 250\n",
    "    if len(df) > sample_size:\n",
    "        df = df.sample(n=sample_size, random_state=42)\n",
    "    documents = df['text'].astype(str).tolist()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "    exit()"
   ],
   "id": "81a5fa81ac100a18",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-29T11:25:43.995056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text, bigram_model=None):\n",
    "    \"\"\"\n",
    "    Enhanced text preprocessing for LDA with bigram support:\n",
    "    1. Comprehensive cleaning (URLs, emails, etc.)\n",
    "    2. Advanced contraction handling\n",
    "    3. Multi-stage filtering (stopwords, length, alphanumeric)\n",
    "    4. Optional bigram processing\n",
    "    \"\"\"\n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')  # Remove line breaks\n",
    "\n",
    "    # Advanced cleaning\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # URLs\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)             # Emails\n",
    "    text = re.sub(r'[^\\w\\s]|\\d', ' ', text)            # Remove punctuation/numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()           # Trim whitespace\n",
    "    text = re.sub(r'\\b(may|would|could|might)\\b', '', text)  # Remove modals\n",
    "    text = re.sub(r'\\b\\w{1,3}\\b', '', text)  # Remove very short words\n",
    "    text = re.sub(r'\\b(?:look|interesting|using|well|find)\\b', '', text)\n",
    "    text = re.sub(r'\\b\\w{5,20}\\b', lambda x: x.group() if x.group() in {\n",
    "        'climate', 'change', 'global', 'warming',\n",
    "        'science', 'data', 'research'\n",
    "    } else '', text)\n",
    "\n",
    "    # Enhanced contraction handling\n",
    "    contractions = {\n",
    "        \"don't\": \"do not\", \"can't\": \"cannot\", \"won't\": \"will not\",\n",
    "        \"it's\": \"it is\", \"i'm\": \"i am\", \"you're\": \"you are\",\n",
    "        \"they're\": \"they are\", \"that's\": \"that is\", \"there's\": \"there is\",\n",
    "        \"he's\": \"he is\", \"she's\": \"she is\", \"what's\": \"what is\"\n",
    "    }\n",
    "    for cont, expanded in contractions.items():\n",
    "        text = text.replace(cont, expanded)\n",
    "\n",
    "    # Tokenization and lemmatization\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [\n",
    "            lemmatizer.lemmatize(token)\n",
    "            for token in tokens\n",
    "            if (token not in stop_words and\n",
    "                3 <= len(token) <= 25 and\n",
    "                token.isalpha())\n",
    "        ]\n",
    "\n",
    "        # Apply bigram model if provided\n",
    "        if bigram_model:\n",
    "            tokens = bigram_model[tokens]\n",
    "\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return []\n",
    "\n",
    "# Preprocess documents\n",
    "processed_docs = [preprocess_text(text) for text in documents]\n",
    "processed_docs = [doc for doc in processed_docs if len(doc) > 0]\n",
    "\n",
    "term_merges = {\n",
    "    'climate': 'climate_science',\n",
    "    'science': 'climate_science',\n",
    "    'global': 'global_warming',\n",
    "    'warming': 'global_warming'\n",
    "}\n",
    "\n",
    "processed_docs = [\n",
    "    [term_merges.get(word, word) for word in doc]\n",
    "    for doc in processed_docs\n",
    "]\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# LDA Model with 3 topics\n",
    "lda_model = LdaModel(\n",
    "    corpus=bow_corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=3,\n",
    "    random_state=42,\n",
    "    passes=50,\n",
    "    alpha='asymmetric'\n",
    ")\n",
    "\n",
    "# Evaluate LDA\n",
    "coherence_model = CoherenceModel(\n",
    "    model=lda_model,\n",
    "    texts=processed_docs,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = lda_model.log_perplexity(bow_corpus)  # This returns bound, need to convert\n",
    "actual_perplexity = 2 ** (-perplexity)  # Convert to actual perplexity\n",
    "\n",
    "print(\"\\nLDA Topics:\")\n",
    "for idx, topic in lda_model.print_topics(-1, num_words=10):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "print(f\"\\nCoherence Score: {coherence_score:.4f}\")\n",
    "print(f\"Perplexity: {actual_perplexity:.4f}\")\n",
    "\n",
    "# Interpretation notes\n",
    "print(\"\\nModel Evaluation Notes:\")\n",
    "print(\"- Higher coherence scores (closer to 1) indicate better topic quality\")\n",
    "print(\"- Lower perplexity scores indicate better model performance\")\n",
    "print(\"- Ideal model has high coherence and low perplexity\")\n",
    "\n",
    "# Test multiple topic numbers\n",
    "coherence_scores = []\n",
    "for num_topics in range(2, 8):\n",
    "    lda = LdaModel(\n",
    "        corpus=bow_corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        passes=30\n",
    "    )\n",
    "    cm = CoherenceModel(\n",
    "        model=lda,\n",
    "        texts=processed_docs,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    coherence_scores.append(cm.get_coherence())\n",
    "\n",
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(2,8), coherence_scores)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.show()"
   ],
   "id": "1918a23b77ff2021",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA Topics:\n",
      "Topic 0: 0.448*\"data\" + 0.210*\"change\" + 0.164*\"climate_science\" + 0.142*\"fuel\" + 0.025*\"global_warming\" + 0.006*\"nasa\" + 0.005*\"research\"\n",
      "Topic 1: 0.713*\"climate_science\" + 0.149*\"change\" + 0.088*\"nasa\" + 0.042*\"research\" + 0.003*\"global_warming\" + 0.002*\"fuel\" + 0.002*\"data\"\n",
      "Topic 2: 0.828*\"global_warming\" + 0.076*\"research\" + 0.072*\"change\" + 0.011*\"climate_science\" + 0.004*\"fuel\" + 0.004*\"nasa\" + 0.004*\"data\"\n",
      "\n",
      "Coherence Score: 0.5077\n",
      "Perplexity: 3.2539\n",
      "\n",
      "Model Evaluation Notes:\n",
      "- Higher coherence scores (closer to 1) indicate better topic quality\n",
      "- Lower perplexity scores indicate better model performance\n",
      "- Ideal model has high coherence and low perplexity\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2c9c938cda9518b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
